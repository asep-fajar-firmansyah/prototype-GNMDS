{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo_GRU_CosineSimilarity_GCN",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vErJHiISK_i4",
        "colab_type": "text"
      },
      "source": [
        "**Synchronize to Google Drive**\n",
        "\n",
        "This feature makes the google colab to connect to the google drive with mounting GDrive and defining the path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQhvn8KgsHSV",
        "colab_type": "code",
        "outputId": "868f6c6a-3269-4691-bea4-cb6477fe364b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Synchronize to google drive, define the root path\n",
        "import os\n",
        "\n",
        "google_colab  = True\n",
        "if google_colab == True:\n",
        "  #This statement used to pointing the google drive storage \n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  root_path = 'gdrive/My Drive/Colab Notebooks/'\n",
        "\n",
        "  #This statement is purposed to import library from the Drive\n",
        "  os.chdir('gdrive/My Drive/Colab Notebooks/')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xggShlKKmbXq",
        "colab_type": "code",
        "outputId": "53efec1b-3bcf-484c-d52e-8e23ce6f39dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, GRU, Activation, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "#from keras.models import Model\n",
        "#from keras.layers import Dense, Input, Dropout, LSTM, Activation, Flatten, GRU, Concatenate\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prFCcPnEqbyc",
        "colab_type": "text"
      },
      "source": [
        "**Hyper Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXl1uHcXqf6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gcn_hidden_layer = 3\n",
        "hidden_size = 300\n",
        "gru_units = 300\n",
        "learning_rate = 0.001\n",
        "batch_size = 1\n",
        "clipnorm = 1.0\n",
        "stop_early = True\n",
        "embedding_size=300\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jldpyssVMSLB",
        "colab_type": "text"
      },
      "source": [
        "**Loading the dataset**\n",
        "\n",
        "Currently, The experiment is using the dataset from DUC2001 and DUC2002. It designed to support Extractive Summarization method in which extract all of documents through pre-processing and store all of sentences to dataset-duc2001 pkl and dataset-duc2002.pkl and each of sentence has labeled based on whether its sentence is similar or exist on the summaries (labeled 1) or no (labeled 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0c44972d-cdb9-4f2b-ef38-451eaa6b33a0",
        "id": "mwkgFuLwH3lw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "import collections\n",
        "import pickle #data preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "nltk.download('punkt')\n",
        "NUM_WORDS=25000\n",
        "counter = collections.Counter()\n",
        "print('Dataset loading ...')\n",
        "#import postprocessing as pr #helper\n",
        "#Step 1 - Load data\n",
        "#Step 1 - Load data\n",
        "with open('datasets/dataset-duc2001.pkl', 'rb') as fp:\n",
        "    data_2001 = pickle.load(fp)\n",
        "\n",
        "with open('datasets/dataset-duc2002.pkl', 'rb') as fp:\n",
        "    data_2002 = pickle.load(fp)\n",
        "\n",
        "clusterSentences_2001, clusterDocuments_2001, clusterSummaries_2001, listY_2001 = data_2001\n",
        "clusterSentences_2002, clusterDocuments_2002, clusterSummaries_2002, listY_2002 = data_2002\n",
        "\n",
        "clusterSentences = clusterSentences_2001 + clusterSentences_2002\n",
        "clusterDocuments = clusterDocuments_2001 + clusterDocuments_2002\n",
        "listY = listY_2001 + listY_2002\n",
        "\n",
        "# Extract to become one bucket of sentences\n",
        "sentences = []\n",
        "for clusterSentence in clusterSentences[:2]:\n",
        "  for sentence in clusterSentence:\n",
        "    sentences.append(sentence)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',lower=True)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "index_word = tokenizer.index_word\n",
        "index_docs = tokenizer.index_docs\n",
        "vocabularySize = len(word_index) + 1\n",
        "\n",
        "#nWord = [len(sentence.split()) for sentence in sentences]\n",
        "#nWord = sorted(nWord,  reverse=True)\n",
        "#words = [sentence.split() for sentence in sentences]\n",
        "#nWord = [len(sentence.split()) for sentence in sentences]\n",
        "#maxlen = max(nWord)\n",
        "sentenceLength = 200"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Dataset loading ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3Yrk8yZAOJM",
        "colab_type": "code",
        "outputId": "ad3b2630-ba01-4667-dfa8-e41385f8b33f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print('total of words :', len(word_index))\n",
        "print('total of sentences', len(sentences))\n",
        "total_documents = 0\n",
        "for docs in clusterDocuments[:2]:\n",
        "  total_documents += len(docs)\n",
        "print('total of documents', total_documents)\n",
        "print('total of clusters', len(clusterSentences[:2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total of words : 5666\n",
            "total of sentences 1790\n",
            "total of documents 22\n",
            "total of clusters 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rCPNnRVSZIM",
        "colab_type": "text"
      },
      "source": [
        "**Load Pre-trained Word2Vec**\n",
        "\n",
        "We use gensim to load Word2Vec which is provided Google. It can be downloaded from https://code.google.com/archive/p/word2vec/\n",
        "\n",
        "Based on the reference paper, the model use 300-dimensional pre-trained word2vec embeddings (Mikolov et al., 2013) as input to GRU$^{sent}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW7ocOcLs1q6",
        "colab_type": "code",
        "outputId": "5e41f6b5-bdac-4a8b-bf2d-ba8af8fc35cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "WORD2VEC_MODEL = \"word-embeddings/word2vec/GoogleNews-vectors-negative300.bin.gz\"\n",
        "word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_MODEL, binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3jANP3eDAeR",
        "colab_type": "text"
      },
      "source": [
        "**Define Y target**\n",
        "\n",
        "Y target defined as categorical data but ideally it should be binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00p3_zy00TNY",
        "colab_type": "code",
        "outputId": "fc745710-fce2-4912-da0e-b852c509694e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)]\n",
        "    return Y\n",
        "    \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "xs, ys = [], []\n",
        "for sentence in sentences:\n",
        "  words = [x.lower() for x in sentence.split()]\n",
        "  wids = [word_index[word] for word in words]\n",
        "  xs.append(wids)\n",
        "\n",
        "ys = []\n",
        "for clusterY in listY[:2]:\n",
        "  for sentenceY in clusterY:\n",
        "    ys.append(sentenceY)\n",
        "\n",
        "X = pad_sequences(xs, maxlen=sentenceLength)\n",
        "Y = np.array(ys)\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.02, random_state=1)\n",
        "\n",
        "#if we use categorical mode\n",
        "Y_train_oh = convert_to_one_hot(Ytrain, C = 2)\n",
        "Y_test_oh = convert_to_one_hot(Ytest, C = 2)\n",
        "\n",
        "\n",
        "#create training and test data based on the clustering document\n",
        "XClusterTrain, XClusterTest, YClusterTrain, YClusterTest = train_test_split(clusterSentences[:2], listY[:2], test_size=0.5, random_state=1)\n",
        "print('XClusterTrain', len(XClusterTrain))\n",
        "print('YClusterTrain', len(YClusterTrain))\n",
        "print('XClusterTest', len(XClusterTest))\n",
        "print('YClusterTest',len(XClusterTest))\n",
        "### if want to reshape the target #############################################\n",
        "#Y_train_oh = Y_train_oh.reshape(Y_train_oh.shape[0], Y_train_oh.shape[1], 1)\n",
        "#Y_test_oh = Y_test_oh.reshape(Y_test_oh.shape[0], Y_test_oh.shape[1], 1)\n",
        "#print(Y_train_oh[0])\n",
        "#print(Y_test_oh[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XClusterTrain 1\n",
            "YClusterTrain 1\n",
            "XClusterTest 1\n",
            "YClusterTest 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmG7pJGJUsOP",
        "colab_type": "text"
      },
      "source": [
        "**Embedding Layer**\n",
        "\n",
        "This function is used to embed pre-trained word2vec as defined before, the function is to produce the matrix from all of the sentences which were mapped to word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7VbvoD5uhoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: pretrained_embedding_layer\n",
        "def pretrained_embedding_matrix(word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
        "    \n",
        "    Arguments:\n",
        "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    embedding_layer -- pretrained layer Keras instance\n",
        "    \"\"\"\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
        "    embedding_matrix = np.zeros((vocab_len, emb_dim))\n",
        "    \n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        try:\n",
        "          embedding_matrix[index, :] = word_to_vec_map[word]\n",
        "        except:\n",
        "          embedding_matrix[index, :] = 0\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G72hOeIIybu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_layer = pretrained_embedding_matrix(word2vec, word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sytdg3VVGUK",
        "colab_type": "text"
      },
      "source": [
        "**GRU$^{sent}$**: Sentence Embedding\n",
        "\n",
        "This model used to produce sentence embedding from a cluster document with $ N $sentences (s$_1$, s$_2$, ... , s$_N$) in total. for each sentence s$_i$ of $ L $ words (w$_1$, w$_2$, ..., w$_L$), GRU$^sent$ recurrently updates hidden states at each time step t:\n",
        "\n",
        "h$_t ^{sent} = GRU^{sent} h_{t-1} ^{sent} . w_t $ ......... (4)\n",
        "\n",
        "where w$_t$ is the word embedding for w$_t$, h$_t ^{sent}$ is the hidden state of GRU$^{sent}$. h$_0$ is initialized as a zero vector, and the input sentence embedding x$_i$ is the last hidden state :\n",
        "\n",
        "x$_i = h_L ^{sent}$ ....................................... (5)\n",
        "\n",
        "All sentence embeddings from the given document cluster are grouped as the node feature matrix X :\n",
        "\n",
        "$X = \n",
        "\\quad\n",
        "\\begin{bmatrix}\n",
        "- X_1 - \\\\\n",
        "- X_2 - \\\\\n",
        ". \\\\\n",
        ". \\\\\n",
        ". \\\\\n",
        "- X_N - \n",
        "\\end{bmatrix}$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnKjpDBcBJ8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "class GruSent(tf.keras.Model):\n",
        "  def __init__(self, vocabulary_size, embedding_size, pretrained_embedding):\n",
        "    super(GruSent, self).__init__()\n",
        "    self.vocabulary_size = vocabulary_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.pretrained_embedding = pretrained_embedding\n",
        "    self.batch_size = batch_size\n",
        "    self.units = gru_units\n",
        "    self.embedding = Embedding(self.vocabulary_size, self.embedding_size, weights=[self.pretrained_embedding], trainable=False)\n",
        "    self.grusent = GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.dense = Dense(1, activation=tf.nn.relu)\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state_h = self.grusent(x, initial_state = hidden)\n",
        "    output = self.dense(output)\n",
        "    return output, state_h\n",
        "  def initialize_hidden_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmQntutEgy9U",
        "colab_type": "code",
        "outputId": "04821ddd-3151-409b-a85e-d640c2e96884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "gruSentModel = GruSent(vocabularySize, embedding_size, embedding_layer)\n",
        "gru_sent_h_0 = gruSentModel.initialize_hidden_state(Xtest.shape[0])\n",
        "print('Xtest', Xtest.shape)\n",
        "print('Initial Gru Sent Hidden', gru_sent_h_0.shape)\n",
        "output, h_L = gruSentModel(Xtest, gru_sent_h_0)\n",
        "print('output', output.shape)\n",
        "print('h_L', h_L.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xtest (36, 200)\n",
            "Initial Gru Sent Hidden (36, 300)\n",
            "output (36, 200, 1)\n",
            "h_L (36, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySw7Svm3p8C4",
        "colab_type": "text"
      },
      "source": [
        "**Graph Convolutional Network (GCN)**\n",
        "\n",
        "The goal of GCN is to learn a function $f(X, A)$ that takes as input :\n",
        "\n",
        "$A \\in R^{N \\times N} $, the adjacency natrix of graph $G$. where $N$ is the number of $G$\n",
        "\n",
        "$X \\in R^{N \\times D} $, the input node feature matrix, where $D$ is the dimension of input node feature vectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaGRoHtOn_9O",
        "colab_type": "text"
      },
      "source": [
        "**Simple GCN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P3PUmUpoDPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load data\n",
        "from networkx import to_numpy_matrix\n",
        "import numpy as np\n",
        "\n",
        "def load_node_feature_matrix(X):\n",
        "    order = sorted(list(X))\n",
        "    return order\n",
        "\n",
        "#gcn layer\n",
        "#stack layer\n",
        "def relu(x):\n",
        "    return np.maximum(x,0) # sample activation function using ReLU(.) = max(0, .)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Normalizing the feature representation\n",
        "The feature representations can be normalized by node degree by transforming the adjacency matrix A by multiplying it with \n",
        "the inverse degree matrix D. Thus our simplified proporagation rule with weights looks like this:\n",
        "f(X,A) = D^-1 * A * H(l) * W(l) or F(X,A) = D^(-0.5) * A * D^(-0.5) * H^(l) * W^(l))\n",
        "Where H^(0) = X\n",
        "\n",
        "f(X,A) has two inputs, as follow:\n",
        "1. A of R^(N*N), the adjacency matrix of graph G, where N is the number of nodes in G\n",
        "2. X of R^(N*D), the input node feature matrix, where D is the dimension of input node feature vectors\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#Layer of GCNs with relu activation function\n",
        "def gcn_layer(A_hat, D_hat, X, W):\n",
        "    #f(X,A) = D^-1 * A * H(l) * W(l) where D^-1 = D^-0.5 * D^-0.5 and H^(0)=X\n",
        "    #print('A_hat shape:',A_hat.shape)\n",
        "    #print('D_hat shape:', D_hat.shape)\n",
        "    #print('X:', X.shape)\n",
        "    #print('W shape:', W.shape)\n",
        "    result = np.matmul(D_hat**-1, A_hat)\n",
        "    result = np.matmul(result, X)\n",
        "    result = np.matmul(result, W)\n",
        "    return relu(result)\n",
        "\n",
        "#Graph Convolutional Networks(GCNs)\n",
        "def gcn(X, graph):\n",
        "    \n",
        "    #load data \n",
        "    order = sorted(list(graph))\n",
        "    A = to_numpy_matrix(graph, nodelist=order) #Return the graph adjacency matrix as a Numpy matrix\n",
        "    #print('A', A.shape)\n",
        "    I = np.eye(graph.number_of_nodes()) #numpy.eye() --> return a 2-D array with ones on the diagonal zeros elsewhere\n",
        "    #print('I', I.shape)\n",
        "    A_hat = A + I #is the adjacency matrix of the undirected graph G with added self-connection\n",
        "    #print('AHat', A_hat.shape)\n",
        "    #applying propagation rules\n",
        "    D_hat = np.array(np.sum(A_hat, axis=0))[0]\n",
        "    D_hat = np.matrix(np.diag(D_hat))\n",
        "   \n",
        "    #print('DHat', D_hat.shape)\n",
        "    #print('X', X.shape)\n",
        "    #determine random weights\n",
        "    W_1 = np.random.normal(loc=0, size=(X.shape[1], 300))\n",
        "    W_2 = np.random.normal(loc=0, size=(W_1.shape[1], 300))\n",
        "    W_3 = np.random.normal(loc=0, size=(W_2.shape[1], 300))\n",
        "    \n",
        "    #Hidden layer 1\n",
        "    H_1 = gcn_layer(A_hat, D_hat, X, W_1) #H^(0)=X\n",
        "    \n",
        "    #Hidden layer 2\n",
        "    H_2 = gcn_layer(A_hat, D_hat, H_1, W_2)\n",
        "\n",
        "    #Hidden layer 3\n",
        "    H_3 = gcn_layer(A_hat, D_hat, H_2, W_3)\n",
        "\n",
        "    output = H_3\n",
        "    \n",
        "    \n",
        "    #Z=f(X,A)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-j905HopxKv",
        "colab_type": "text"
      },
      "source": [
        "**Cluster Embedding**\n",
        "\n",
        "The model apply a second-level RNN $GRU^{doc}$ to encode the entire document cluster. Given a document cluster $C$ with $M$ documents $(d_1, d_2, ..., d_M)$, for document $d_i$ with $|d_i|$ sentences, $GRU^{doc}$ first builds the document embedding $d_i$ on top of sentence embeddings:\n",
        "\n",
        "$h_t^{doc} = GRU^{doc}(h_{t-1}^{doc}, s_t)$ ...... (8)\n",
        "\n",
        "$d_i = h_{d_i}^{doc} $ ..................................... (9)\n",
        "\n",
        "where $s_t$ is the sentence embedding in the document d_i. In Eq. 9, we extract the last hidden state as the document embedding for $d_i$. \n",
        "\n",
        "In Eq. 10, we average over document embeddings to produce the cluster embedding $C$\n",
        "\n",
        "$C = \\frac1M $$\\sum_{i=1}^M d_i$ ......................... (10)\n",
        "\n",
        "All the GRU used are forward.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_o3-s-7punY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "class GruDoc(tf.keras.Model):\n",
        "  def __init__(self, vocabularySize, embedding_size):\n",
        "    super(GruDoc, self).__init__()\n",
        "    self.vocabulary_size = vocabularySize\n",
        "    self.embedding_size = embedding_size\n",
        "    self.units = gru_units\n",
        "    self.embedding = Embedding(self.vocabulary_size, self.embedding_size, trainable=False)\n",
        "    self.grudoc = GRU(self.units,\n",
        "                      return_sequences=False,\n",
        "                      return_state=False,\n",
        "                      recurrent_initializer='glorot_uniform')\n",
        "    self.dense = Dense(1, activation=tf.nn.relu)\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    # Should apply document embedding \n",
        "    x = self.embedding(x)\n",
        "    output = self.grudoc(x, initial_state = hidden)\n",
        "    output = self.dense(output)\n",
        "    return output\n",
        "  def initialize_hidden_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kCjuGZhz2IZ",
        "colab_type": "code",
        "outputId": "855c34df-fb76-4835-ec9e-553bee4099e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#testing the model\n",
        "gruDocModel = GruDoc(vocabularySize, embedding_size)\n",
        "gru_doc_h_0 = gruDocModel.initialize_hidden_state(Xtest.shape[0])\n",
        "print('Initial Gru Doc h_0', Xtest.shape)\n",
        "print('gru Sent last Hidden', )\n",
        "gru_doc_output = gruDocModel(h_L, gru_doc_h_0)\n",
        "print('output', gru_doc_output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial Gru Doc h_0 (36, 200)\n",
            "gru Sent last Hidden\n",
            "output (36, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eAfcst7xBYY",
        "colab_type": "text"
      },
      "source": [
        "**Attention Model**\n",
        "\n",
        "This function is implementing attention model from bahdanau \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HduKI4o3xGfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    salienceScores = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = salienceScores * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, salienceScores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWq7rHkUT7Gh",
        "colab_type": "code",
        "outputId": "8a642b8b-c72e-418f-a238-8fab6f640a6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def transformToVector(ClusterSentences):\n",
        "  xs_cluster=[]\n",
        "  for sentences in ClusterSentences:\n",
        "    xs = []\n",
        "    for sentence in sentences:\n",
        "      words = [x.lower() for x in sentence.split()]\n",
        "      wids = [word_index[word] for word in words]\n",
        "      xs.append(wids)\n",
        "    xs_pad = tf.keras.preprocessing.sequence.pad_sequences(xs, maxlen=sentenceLength, padding='post')  \n",
        "    xs_cluster.append(xs_pad)\n",
        "  return xs_cluster\n",
        "XS_train = transformToVector(XClusterTrain)\n",
        "YS_train = []\n",
        "for clusterY in YClusterTrain:\n",
        "  ys = []\n",
        "  for sentenceY in clusterY:\n",
        "    ys.append(sentenceY)\n",
        "  YS_train.append(ys)\n",
        "print(len(XS_train))\n",
        "print(len(YS_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEhxe1WkWvwr",
        "colab_type": "code",
        "outputId": "243d7311-24dc-4070-d651-23ab34e3134a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "XS_test = transformToVector(XClusterTest)\n",
        "YS_test = []\n",
        "for clusterY in YClusterTest:\n",
        "  ys = []\n",
        "  for sentenceY in clusterY:\n",
        "    ys.append(sentenceY)\n",
        "  YS_test.append(ys)\n",
        "print(len(XS_test))\n",
        "print(len(YS_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJeludkmRV8z",
        "colab_type": "code",
        "outputId": "f83ceb32-c6c0-4d93-9a35-8be48a59778c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "XS_train_sentences = []\n",
        "for xs_cluster in XS_train:\n",
        "  for xs_sentence in xs_cluster:\n",
        "    XS_train_sentences.append(xs_sentence)\n",
        "\n",
        "XS_test_sentences = []\n",
        "for xs_cluster in XS_test:\n",
        "  for xs_sentence in xs_cluster:\n",
        "    XS_test_sentences.append(xs_sentence)\n",
        "\n",
        "YS_train_labels = []\n",
        "for ys_cluster in YS_train:\n",
        "  for ys_label in ys_cluster:\n",
        "    YS_train_labels.append(ys_label)\n",
        "\n",
        "YS_test_labels = []\n",
        "for ys_cluster in YS_test:\n",
        "  for ys_label in ys_cluster:\n",
        "    YS_test_labels.append(ys_label) \n",
        "\n",
        "print('XS_train_sentences', len(XS_train_sentences))\n",
        "print('XS_test_sentences', len(XS_test_sentences))\n",
        "print('YS_train_labels', len(YS_train_labels))\n",
        "print('YS_test_labels', len(YS_test_labels)) \n",
        "YS_train_labels = tf.convert_to_tensor(YS_train_labels)\n",
        "XS_train_sentences = tf.convert_to_tensor(XS_train_sentences)\n",
        "XS_test_sentences = tf.convert_to_tensor(XS_test_sentences)\n",
        "YS_test_labels = tf.convert_to_tensor(YS_test_labels)\n",
        "print('XS train sentences shape', XS_train_sentences.shape)\n",
        "print('YS train labels shape', YS_train_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XS_train_sentences 1106\n",
            "XS_test_sentences 684\n",
            "YS_train_labels 1106\n",
            "YS_test_labels 684\n",
            "XS train sentences shape (1106, 200)\n",
            "YS train labels shape (1106,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2JLMNO4e74_",
        "colab_type": "text"
      },
      "source": [
        "**Graph Representation of Cluster**\n",
        "\n",
        "Cosine Similary (Erkan and Radev)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbvT48A5jU0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from lib.lexrank import lexrank\n",
        "#from lib.simpleGCN import gcn\n",
        "def getSentences(clusterSents):\n",
        "  sentences=[]\n",
        "  for cluster in clusterSents:\n",
        "    for sentence in cluster:\n",
        "      sentences.append(sentence)\n",
        "  return sentences\n",
        "\n",
        "train_sentences = getSentences(XClusterTrain)\n",
        "test_sentences = getSentences(XClusterTest)\n",
        "\n",
        "lx_train_sentences, lx_train_num, lxrank_test, graph_train = lexrank(train_sentences, word_split=True)\n",
        "lx_test_sentences, lx_test_num, lxrank_test, graph_test = lexrank(test_sentences, word_split=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hK6xqd5IHXi",
        "colab_type": "code",
        "outputId": "ce23a83b-5061-48b4-fc44-bf9377b45f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# test SimpleGCN\n",
        "gruSentModel = GruSent(vocabularySize, embedding_size, embedding_layer)\n",
        "gru_sent_h_0 = gruSentModel.initialize_hidden_state(XS_train_sentences.shape[0])\n",
        "#print('Xtest', Xtest.shape)\n",
        "#print('Initial Gru Sent Hidden', gru_sent_h_0.shape)\n",
        "output= gruSentModel(XS_train_sentences, gru_sent_h_0)\n",
        "#print('output', output.shape)\n",
        "#print('h_L', h_L.shape)\n",
        "#h_L_T = tf.transpose(h_L)\n",
        "final_sentence_embedding_train = gcn(output, graph_train)\n",
        "print('Final Sentence Embedding Train', final_sentence_embedding_train.shape)\n",
        "\n",
        "gruDocModel = GruDoc(vocabularySize, embedding_size)\n",
        "gru_doc_h_0 = gruDocModel.initialize_hidden_state(final_sentence_embedding_train.shape[0])\n",
        "print('Initial Gru Doc h_0', gru_doc_h_0.shape)\n",
        "gru_doc_output = gruDocModel(final_sentence_embedding_train, gru_doc_h_0)\n",
        "print('output', gru_doc_output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Sentence Embedding Train (1106, 300)\n",
            "Initial Gru Doc h_0 (1106, 300)\n",
            "output (1106, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7H78DIexLOh",
        "colab_type": "text"
      },
      "source": [
        "**Define Optimazation and Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSHru5dHQvnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPdC_EFZREdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(sentences, labels, initial_hidden):\n",
        "  with tf.GradientTape() as tape:\n",
        "    output, h_L = gruSentModel(sentences, initial_hidden)\n",
        "    #final_sentence_embedding_train = gcn(h_L, graph_train)\n",
        "    predictions = gruDocModel(h_L, initial_hidden)\n",
        "    loss = loss_object(labels, predictions)\n",
        "    \n",
        "  gradients = tape.gradient(loss, gruDocModel.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, gruDocModel.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03gUKFu9RMt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def test_step(sentences, labels, initial_hidden):\n",
        "  output, h_L  = gruSentModel(sentences, initial_hidden)\n",
        "  #final_sentence_embedding_train = gcn(h_L, graph_train)\n",
        "  predictions = gruDocModel(h_L, initial_hidden)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "  \n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_7XjHfGnLur",
        "colab_type": "text"
      },
      "source": [
        "**Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmhLrR_A5Ngv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((XS_train_sentences, YS_train_labels)).batch(10)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((XS_test_sentences, YS_test_labels)).batch(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STwhKYOFYRl_",
        "colab_type": "code",
        "outputId": "b71d9918-b2c9-4ac5-e97a-b12a463d247f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for sents, labels in train_ds:\n",
        "    #print('sents', sents.shape)\n",
        "    initial_hidden = gruSentModel.initialize_hidden_state(sents.shape[0])\n",
        "    train_step(sents, labels, initial_hidden)\n",
        "\n",
        "  for test_sentences, test_labels in test_ds:\n",
        "    initial_hidden = gruSentModel.initialize_hidden_state(test_sentences.shape[0])\n",
        "    test_step(test_sentences, test_labels, initial_hidden)\n",
        "  \n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "\n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.5302756428718567, Accuracy: 95.20003509521484, Test Loss: 0.7824252247810364, Test Accuracy: 94.92755889892578\n",
            "Epoch 2, Loss: 0.5141651630401611, Accuracy: 96.66667938232422, Test Loss: 0.7824252247810364, Test Accuracy: 94.92755889892578\n",
            "Epoch 3, Loss: 0.5141651630401611, Accuracy: 96.66667938232422, Test Loss: 0.7824252247810364, Test Accuracy: 94.92755889892578\n",
            "Epoch 4, Loss: 0.5141651630401611, Accuracy: 96.66667938232422, Test Loss: 0.7824252247810364, Test Accuracy: 94.92755889892578\n",
            "Epoch 5, Loss: 0.5141651630401611, Accuracy: 96.66667938232422, Test Loss: 0.7824252247810364, Test Accuracy: 94.92755889892578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU1Hd1vdL7il",
        "colab_type": "code",
        "outputId": "49873888-e630-4669-f6cc-0583a36b9e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def loss(model, x, y):\n",
        "  y_ = model(x)\n",
        "\n",
        "  return loss_object(y_true=y, y_pred=y_)\n",
        "\n",
        "\n",
        "l = loss(gruSentModel, XS_train_sentences, YS_train_labels)\n",
        "print(\"Loss test: {}\".format(l))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss test: 0.5157886147499084\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}