# -*- coding: utf-8 -*-
"""PreProcessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qwqDGsD4oFZg3rfgOMvUdPWxibW5Nx1-
"""

# Synchronize to google drive, define the root path
import os

google_colab  = True
if google_colab == True:
  #This statement used to pointing the google drive storage 
  from google.colab import drive
  drive.mount('/content/gdrive')
  root_path = 'gdrive/My Drive/Colab Notebooks/'

  #This statement is purposed to import library from the Drive
  os.chdir('gdrive/My Drive/Colab Notebooks/')

import numpy as np
import pickle
import os
import re
import nltk
from nltk import tokenize
nltk.download('punkt')

def extractText(path):
    f = open(path, "r")

    fullText = f.read().replace("\n", " ")
    f.close()        
    sentences = ""
    textIndex = fullText.find("<TEXT>")
    # extracts the text in the documents
    # looks for the <TEXT> and </TEXT> tags
    while textIndex != -1: 
        sentences += fullText[textIndex + 6 : fullText.find("</TEXT>", textIndex) ]
        textIndex = fullText.find("<TEXT>", textIndex + 1)

    #old = sentences
    sentences = sentences.replace("<P>", " ")
    sentences = sentences.replace("</P>", " ")
    #xmlRegex = re.compile("<.*?>.*?</.*?>|<.*?/>", re.IGNORECASE)
    #sentences = xmlRegex.sub("<.*?>.*?</.*?>|<.*?/>", sentences)
    #print(old, sentences)
    sentences = sentences.replace(";", " ")
    tokenizeSentences = tokenize.sent_tokenize(sentences)
    finalSentences = []
    for sentence in tokenizeSentences:
      sent = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", sentence)
      sent = re.sub(' +', ' ',sent)
      sent = sent.lstrip()
      sent = re.sub(r'[^\w\s]','',sent)
      if sent != ".":
        finalSentences.append(sent)
    return finalSentences

def parsePerdocs(path):
    f = open(path, "r")
    
    # load all lines into a single string
    fullText = f.read().replace("\n", " ")
    f.close()
    
    # nltk attempt
    summaries = {} # { docID : summary }
    sumIndex = fullText.find("DOCREF=")
   
    # gets all of the summaries and stores them in the appropriate places 
    while sumIndex != -1:
        docID = fullText[sumIndex + 8:fullText.find("\"", sumIndex + 9)]
        
        startSum = fullText.find(">", sumIndex)
        endSum = fullText.find("</SUM>", sumIndex)

        text = fullText[startSum + 1:endSum]
        text = text.replace("<P>", " ")
        text = text.replace("</P>", " ")

        summaries[docID] = text

        sumIndex = fullText.find("DOCREF=", endSum) 
    
    for k in summaries.keys():
        tokenizeSentences = tokenize.sent_tokenize(summaries[k])
        finalSentences = []
        for sentence in tokenizeSentences:
          sent = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", sentence)
          sent = re.sub(' +', ' ',sent)
          sent = sent.lstrip()
          sent = re.sub(r'[^\w\s]','',sent)
          if sent != ".":
            finalSentences.append(sent)
        summaries[k] = finalSentences

    return summaries

def transformClusterDocsToSentences(docClustersPkl):
  ## break down cluster document to sentences
  cluster_sentences=[]
  for cluster in docClustersPkl.keys():
    sentenceList=[]
    docs = docClustersPkl[cluster]
    for doc_id in docs.keys():
      sentences = docs[doc_id]
      for sentence in sentences:
        sentenceLength = len(sentence.split())
        if sentenceLength > 1:
          if sentenceLength > 200:
            sentence = sentence.split(' ')[:200]
            sentence = ' '.join(sentence)
          sentenceList.append(sentence)
    cluster_sentences.append(sentenceList)
  return cluster_sentences

def transformClusterSummaryToList(clusterSummariesPkl):
  summariesList=[]
  for cluster in clusterSummariesPkl.keys():
    summariesList.append(clusterSummariesPkl[cluster])
  return summariesList

# Transform Clusters Document to Cluster Sentences based on document group
def transformToSentecesGroupByDoc(docClustersPkl):
  docClusterList=[]
  for cluster in docClustersPkl.keys():
    docsCluster = docClustersPkl[cluster]
    docSentenceList=[]
    for docId in docsCluster.keys():
      docSentences=docsCluster[docId]
      sentenceList=[]
      for sentence in docSentences:
        sentenceLength = len(sentence.split()) 
        if sentenceLength > 1:
          if sentenceLength > 200:
            sentence = sentence.split(' ')[:200]
            sentence = ' '.join(sentence)
          sentenceList.append(sentence) 
      docSentenceList.append(sentenceList)    
    docClusterList.append(docSentenceList)
  return docClusterList

# Check the sentence, is it the important sentence in Summaries?
def distance_unigrams_same(t1, t2):
    """Unigram distance metric, term frequency is ignored,
       0 if unigrams are identical, 1.0 if no unigrams are common"""
    t1_terms = make_terms_from_string(t1)
    t2_terms = make_terms_from_string(t2)
    terms1 = set(t1_terms)
    terms2 = set(t2_terms)
    shared_terms = terms1.intersection(terms2)
    #print(shared_terms)
    all_terms = terms1.union(terms2)
    #print(all_terms)
    dist = 1.0 - (len(shared_terms) / float(len(all_terms)))
    return dist


def distance_bigrams_same(t1, t2):
    """Bigram distance metric, term frequency is ignored,
       0 if bigrams are identical, 1.0 if no bigrams are common"""
    t1_terms = make_terms_from_string(t1)
    t2_terms = make_terms_from_string(t2)
    terms1 = set(ngrams(t1_terms, 2)) # was using nltk.bigrams
    terms2 = set(ngrams(t2_terms, 2))
    shared_terms = terms1.intersection(terms2)
    #print(shared_terms)
    all_terms = terms1.union(terms2)
    #print(all_terms)
    dist = 1.0
    if len(all_terms) > 0:
        dist = 1.0 - (len(shared_terms) / float(len(all_terms)))
    return dist

def make_terms_from_string(s):
    """turn string s into a list of unicode terms"""
    u = s
    return u.split()

def ngrams(sequence, n):
    """Create ngrams from sequence, e.g. ([1,2,3], 2) -> [(1,2), (2,3)]
       Note that fewer sequence items than n results in an empty list being returned"""
    # credit: http://stackoverflow.com/questions/2380394/simple-implementation-of-n-gram-tf-idf-and-cosine-similarity-in-python
    sequence = list(sequence)
    count = max(0, len(sequence) - n + 1)
    return [tuple(sequence[i:i+n]) for i in range(count)]

def joinSentenceSummary(pkl_summaries):
  listSentenceSummaries = []
  for sentencesSummary in pkl_summaries:
    joinSentenceSummaries = ""
    for sentenceSummary in sentencesSummary:
      joinSentenceSummaries +=sentenceSummary
      joinSentenceSummaries +=" "
    listSentenceSummaries.append(joinSentenceSummaries)
  return listSentenceSummaries

def similarityMeasure(pkl_clusters, listSentenceSummaries):
  i=0
  cluster=0
  clusterY=[]
  #print(len(pkl_clusters))
  for clusterSentences in pkl_clusters:
    cluster +=1
    #print(cluster)
    x = listSentenceSummaries[i]
    no=0
    clusterDocY=[]
    for sentence in clusterSentences:
      no += 1
      score=1
      z = sentence
      unigram_dist = distance_unigrams_same(z, x)
      bigram_dist = distance_bigrams_same(z, x)
      if unigram_dist < score:
        score=unigram_dist
            
      if (bigram_dist < score):
        score=bigram_dist
              
      if (score <= 0.89):
        similarity=1
      else:
        similarity=0
      clusterDocY.append(similarity)
    clusterY.append(clusterDocY)
    i += 1
  return clusterY

def loadData(dataRoot, summarySize=0):
  sentences = {}
  summaries = {}   
  documentClusters = {}
  documentMembers = {}
  clusterSummary = {}
  # gets the raw documents 
  rawDocs = dataRoot + "/docs/"
  walker = os.walk(rawDocs)
  for x in sorted(walker):
    #print(x)
    path = x[0]
    #print(path)
    dirs = sorted(x[1])
    #print(dirs)
    files = x[2]
    if len(dirs) != 0:
      continue 
    nClusterSentences = 0
    n_docs = 0
    docsId = []
    documents = {}
    for f in files:
      print("file:", path + "/" + f, end="\r")
      pathCluster = path.split('/')
      document = extractText(path + "/" + f)
      sentences[f] = document
      documents[f] = document
      docsId.append(f)
    
    if len(docsId) > 0:
      cluster = pathCluster[len(pathCluster)-1]
      documentClusters[cluster] = docsId
      documentMembers[cluster] = documents

  # gets the summaries
  rawSummaries = dataRoot + "/summaries/"
  walker = os.walk(rawSummaries)

  for x in sorted(walker):
    path = x[0]
    #print(path)
    dirs = sorted(x[1])
    #print(dirs)
    files = x[2]

    if len(dirs) != 0:
      continue 

    if summarySize != "" :
      tmpSummaries = parsePerdocs(path + "/" + summarySize)
      path_b = path.split('/')
      cluster = path_b[len(path_b)-1]
      for k in tmpSummaries.keys():
        summaries[k] = tmpSummaries[k]
      clusterSummary[cluster] = summaries[k]
    else:
      for f in files:
        tmpSummaries = parsePerdocs(path + "/" + f)
        pathCluster = path.split('/')
        cluster = pathCluster[len(pathCluster)-1]
        for k in tmpSummaries.keys():
          summaries[k] = tmpSummaries[k]
        clusterSummary[cluster] = summary[k]
  clusterSentences = transformClusterDocsToSentences(documentMembers)
  clusterSentencesByDoc = transformToSentecesGroupByDoc(documentMembers)
  clusterSummary = transformClusterSummaryToList(clusterSummary)
  #joinSentenceSummary
  listSummarySentence = joinSentenceSummary(clusterSummary)
  listY = similarityMeasure(clusterSentences, listSummarySentence)
  return clusterSentences, clusterSentencesByDoc, listSummarySentence , listY

# Hint to access duc-test
# A sample of cluster document --> d01a
# dataRoot = 'datasets/duc-test/d01a'

# Hint to access duc2002
dataRoot = 'datasets/duc2002'

clusters, documents, summaries, listY= loadData(dataRoot, '100')

for cluster in clusters:
  for sentence in cluster:
    if len(sentence.split()) < 3:
      print (sentence)
      print(len(sentence))

sentenceList = []
for cluster in clusters:
  for sentence in cluster:
    sentenceList.append(sentence)
print(len(sentenceList))

nWord = [len(sentence.split()) for sentence in sentenceList]
nWord = sorted(nWord)
print(nWord[:10])

import pickle #data preprocessing
# Store multiple data on file .pkl
with open('datasets/dataset-duc2002.pkl', 'wb') as file_pi:
        pickle.dump((clusters, documents, summaries, listY), file_pi, -1)

import pickle #data preprocessing
with open('datasets/dataset-duc2002.pkl', 'rb') as fp:
    data = pickle.load(fp)

