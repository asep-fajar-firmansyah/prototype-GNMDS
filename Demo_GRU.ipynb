{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo-GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vErJHiISK_i4",
        "colab_type": "text"
      },
      "source": [
        "**Synchronize to Google Drive**\n",
        "\n",
        "This feature makes the google colab to connect to the google drive with mounting GDrive and defining the path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQhvn8KgsHSV",
        "colab_type": "code",
        "outputId": "d3d30a62-86d9-4a33-b06b-9a160a7c089e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# Synchronize to google drive, define the root path\n",
        "import os\n",
        "\n",
        "google_colab  = True\n",
        "if google_colab == True:\n",
        "  #This statement used to pointing the google drive storage \n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  root_path = 'gdrive/My Drive/Colab Notebooks/'\n",
        "\n",
        "  #This statement is purposed to import library from the Drive\n",
        "  os.chdir('gdrive/My Drive/Colab Notebooks/')\n",
        "  "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xggShlKKmbXq",
        "colab_type": "code",
        "outputId": "dc3b5ccc-2eb9-4b31-b7ec-c903653704ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, GRU, Activation, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "#from keras.models import Model\n",
        "#from keras.layers import Dense, Input, Dropout, LSTM, Activation, Flatten, GRU, Concatenate\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "np.random.seed(1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prFCcPnEqbyc",
        "colab_type": "text"
      },
      "source": [
        "**Hyper Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXl1uHcXqf6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gcn_hidden_layer = 3\n",
        "hidden_size = 300\n",
        "gru_units = 300\n",
        "learning_rate = 0.001\n",
        "batch_size = 1\n",
        "clipnorm = 1.0\n",
        "stop_early = True\n",
        "embedding_size=300\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jldpyssVMSLB",
        "colab_type": "text"
      },
      "source": [
        "**Loading the dataset**\n",
        "\n",
        "Currently, The experiment is using the dataset from DUC2001 and DUC2002. It designed to support Extractive Summarization method in which extract all of documents through pre-processing and store all of sentences to dataset-duc2001 pkl and dataset-duc2002.pkl and each of sentence has labeled based on whether its sentence is similar or exist on the summaries (labeled 1) or no (labeled 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cfa1b4ec-1362-45b7-8bd9-4d6e8cb2f6b0",
        "id": "mwkgFuLwH3lw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "import collections\n",
        "import pickle #data preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "nltk.download('punkt')\n",
        "NUM_WORDS=25000\n",
        "counter = collections.Counter()\n",
        "print('Dataset loading ...')\n",
        "#import postprocessing as pr #helper\n",
        "#Step 1 - Load data\n",
        "#Step 1 - Load data\n",
        "with open('datasets/dataset-duc2001.pkl', 'rb') as fp:\n",
        "    data_2001 = pickle.load(fp)\n",
        "\n",
        "with open('datasets/dataset-duc2002.pkl', 'rb') as fp:\n",
        "    data_2002 = pickle.load(fp)\n",
        "\n",
        "clusterSentences_2001, clusterDocuments_2001, clusterSummaries_2001, listY_2001 = data_2001\n",
        "clusterSentences_2002, clusterDocuments_2002, clusterSummaries_2002, listY_2002 = data_2002\n",
        "\n",
        "clusterSentences = clusterSentences_2001 + clusterSentences_2002\n",
        "clusterDocuments = clusterDocuments_2001 + clusterDocuments_2002\n",
        "clusterSummaries = clusterSummaries_2001 + clusterSummaries_2002\n",
        "listY = listY_2001 + listY_2002\n",
        "\n",
        "# Extract to become one bucket of sentences\n",
        "sentences = []\n",
        "for clusterSentence in clusterSentences[:2]:\n",
        "  for sentence in clusterSentence:\n",
        "    sentences.append(sentence)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',lower=True)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "index_word = tokenizer.index_word\n",
        "index_docs = tokenizer.index_docs\n",
        "vocabularySize = len(word_index) + 1\n",
        "\n",
        "#nWord = [len(sentence.split()) for sentence in sentences]\n",
        "#nWord = sorted(nWord,  reverse=True)\n",
        "#words = [sentence.split() for sentence in sentences]\n",
        "#nWord = [len(sentence.split()) for sentence in sentences]\n",
        "#maxlen = max(nWord)\n",
        "sentenceLength = 200"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Dataset loading ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3Yrk8yZAOJM",
        "colab_type": "code",
        "outputId": "51e84330-6087-4941-c1f3-e7e293c988ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print('total of words :', len(word_index))\n",
        "print('total of sentences', len(sentences))\n",
        "total_documents = 0\n",
        "for docs in clusterDocuments[:2]:\n",
        "  total_documents += len(docs)\n",
        "print('total of documents', total_documents)\n",
        "print('total of clusters', len(clusterSentences[:2]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total of words : 5666\n",
            "total of sentences 1790\n",
            "total of documents 22\n",
            "total of clusters 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rCPNnRVSZIM",
        "colab_type": "text"
      },
      "source": [
        "**Load Pre-trained Word2Vec**\n",
        "\n",
        "We use gensim to load Word2Vec which is provided Google. It can be downloaded from https://code.google.com/archive/p/word2vec/\n",
        "\n",
        "Based on the reference paper, the model use 300-dimensional pre-trained word2vec embeddings (Mikolov et al., 2013) as input to GRU$^{sent}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW7ocOcLs1q6",
        "colab_type": "code",
        "outputId": "ee087f49-a7b7-411f-8f90-3627e73bc935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "WORD2VEC_MODEL = \"word-embeddings/word2vec/GoogleNews-vectors-negative300.bin.gz\"\n",
        "word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_MODEL, binary=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3jANP3eDAeR",
        "colab_type": "text"
      },
      "source": [
        "**Define Y target**\n",
        "\n",
        "Y target defined as categorical data but ideally it should be binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00p3_zy00TNY",
        "colab_type": "code",
        "outputId": "3549cbff-6a64-4780-edaf-048e010a7bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)]\n",
        "    return Y\n",
        "    \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "xs, ys = [], []\n",
        "for sentence in sentences:\n",
        "  words = [x.lower() for x in sentence.split()]\n",
        "  wids = [word_index[word] for word in words]\n",
        "  xs.append(wids)\n",
        "\n",
        "ys = []\n",
        "for clusterY in listY[:2]:\n",
        "  for sentenceY in clusterY:\n",
        "    ys.append(sentenceY)\n",
        "\n",
        "X = pad_sequences(xs, maxlen=sentenceLength)\n",
        "Y = np.array(ys)\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.02, random_state=1)\n",
        "\n",
        "#if we use categorical mode\n",
        "Y_train_oh = convert_to_one_hot(Ytrain, C = 2)\n",
        "Y_test_oh = convert_to_one_hot(Ytest, C = 2)\n",
        "\n",
        "\n",
        "#create training and test data based on the clustering document\n",
        "XClusterTrain, XClusterTest, YClusterTrain, YClusterTest = train_test_split(clusterSentences[:2], listY[:2], test_size=0.5, random_state=1)\n",
        "print('XClusterTrain', len(XClusterTrain))\n",
        "print('YClusterTrain', len(YClusterTrain))\n",
        "print('XClusterTest', len(XClusterTest))\n",
        "print('YClusterTest',len(XClusterTest))\n",
        "### if want to reshape the target #############################################\n",
        "#Y_train_oh = Y_train_oh.reshape(Y_train_oh.shape[0], Y_train_oh.shape[1], 1)\n",
        "#Y_test_oh = Y_test_oh.reshape(Y_test_oh.shape[0], Y_test_oh.shape[1], 1)\n",
        "#print(Y_train_oh[0])\n",
        "#print(Y_test_oh[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XClusterTrain 1\n",
            "YClusterTrain 1\n",
            "XClusterTest 1\n",
            "YClusterTest 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmG7pJGJUsOP",
        "colab_type": "text"
      },
      "source": [
        "**Embedding Layer**\n",
        "\n",
        "This function is used to embed pre-trained word2vec as defined before, the function is to produce the matrix from all of the sentences which were mapped to word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7VbvoD5uhoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: pretrained_embedding_layer\n",
        "def pretrained_embedding_matrix(word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
        "    \n",
        "    Arguments:\n",
        "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    embedding_layer -- pretrained layer Keras instance\n",
        "    \"\"\"\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
        "    embedding_matrix = np.zeros((vocab_len, emb_dim))\n",
        "    \n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        try:\n",
        "          embedding_matrix[index, :] = word_to_vec_map[word]\n",
        "        except:\n",
        "          embedding_matrix[index, :] = 0\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G72hOeIIybu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_layer = pretrained_embedding_matrix(word2vec, word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sytdg3VVGUK",
        "colab_type": "text"
      },
      "source": [
        "**GRU$^{sent}$**: Sentence Embedding\n",
        "\n",
        "This model used to produce sentence embedding from a cluster document with $ N $sentences (s$_1$, s$_2$, ... , s$_N$) in total. for each sentence s$_i$ of $ L $ words (w$_1$, w$_2$, ..., w$_L$), GRU$^sent$ recurrently updates hidden states at each time step t:\n",
        "\n",
        "h$_t ^{sent} = GRU^{sent} h_{t-1} ^{sent} . w_t $ ......... (4)\n",
        "\n",
        "where w$_t$ is the word embedding for w$_t$, h$_t ^{sent}$ is the hidden state of GRU$^{sent}$. h$_0$ is initialized as a zero vector, and the input sentence embedding x$_i$ is the last hidden state :\n",
        "\n",
        "x$_i = h_L ^{sent}$ ....................................... (5)\n",
        "\n",
        "All sentence embeddings from the given document cluster are grouped as the node feature matrix X :\n",
        "\n",
        "$X = \n",
        "\\quad\n",
        "\\begin{bmatrix}\n",
        "- X_1 - \\\\\n",
        "- X_2 - \\\\\n",
        ". \\\\\n",
        ". \\\\\n",
        ". \\\\\n",
        "- X_N - \n",
        "\\end{bmatrix}$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnKjpDBcBJ8T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "48586d3b-bd16-4c3e-8182-1577cabad7b6"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "import tensorflow.keras.regularizers as kr\n",
        "class GruSent(tf.keras.Model):\n",
        "  def __init__(self, vocabulary_size, embedding_size, pretrained_embedding):\n",
        "    super(GruSent, self).__init__()\n",
        "    self.vocabulary_size = vocabulary_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.pretrained_embedding = pretrained_embedding\n",
        "    self.batch_size = batch_size\n",
        "    self.units = gru_units\n",
        "    self.embedding = Embedding(self.vocabulary_size, self.embedding_size, weights=[self.pretrained_embedding], trainable=False)\n",
        "    kernel_regularizer = kr.l1(0.01)\n",
        "    self.grusent = GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   kernel_regularizer=kernel_regularizer\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.dense = Dense(1, activation=tf.nn.relu)\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state_h = self.grusent(x, initial_state = hidden)\n",
        "    output = self.dense(output)\n",
        "    return output, state_h\n",
        "  def initialize_hidden_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-3f6f856bd79e>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    recurrent_initializer='glorot_uniform')\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmQntutEgy9U",
        "colab_type": "code",
        "outputId": "4d125bce-7477-475f-9d93-bfb9f1741c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "gruSentModel = GruSent(vocabularySize, embedding_size, embedding_layer)\n",
        "gru_sent_h_0 = gruSentModel.initialize_hidden_state(Xtest.shape[0])\n",
        "print('Xtest', Xtest.shape)\n",
        "print('Initial Gru Sent Hidden', gru_sent_h_0.shape)\n",
        "output, h_L = gruSentModel(Xtest, gru_sent_h_0)\n",
        "print('output', output.shape)\n",
        "print('h_L', h_L.shape)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xtest (36, 200)\n",
            "Initial Gru Sent Hidden (36, 300)\n",
            "output (36, 200, 1)\n",
            "h_L (36, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-j905HopxKv",
        "colab_type": "text"
      },
      "source": [
        "**Cluster Embedding**\n",
        "\n",
        "The model apply a second-level RNN $GRU^{doc}$ to encode the entire document cluster. Given a document cluster $C$ with $M$ documents $(d_1, d_2, ..., d_M)$, for document $d_i$ with $|d_i|$ sentences, $GRU^{doc}$ first builds the document embedding $d_i$ on top of sentence embeddings:\n",
        "\n",
        "$h_t^{doc} = GRU^{doc}(h_{t-1}^{doc}, s_t)$ ...... (8)\n",
        "\n",
        "$d_i = h_{d_i}^{doc} $ ..................................... (9)\n",
        "\n",
        "where $s_t$ is the sentence embedding in the document d_i. In Eq. 9, we extract the last hidden state as the document embedding for $d_i$. \n",
        "\n",
        "In Eq. 10, we average over document embeddings to produce the cluster embedding $C$\n",
        "\n",
        "$C = \\frac1M $$\\sum_{i=1}^M d_i$ ......................... (10)\n",
        "\n",
        "All the GRU used are forward.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_o3-s-7punY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "class GruDoc(tf.keras.Model):\n",
        "  def __init__(self, vocabularySize, embedding_size):\n",
        "    super(GruDoc, self).__init__()\n",
        "    self.vocabulary_size = vocabularySize\n",
        "    self.embedding_size = embedding_size\n",
        "    self.units = gru_units\n",
        "    self.embedding = Embedding(self.vocabulary_size, self.embedding_size, trainable=False)\n",
        "    self.grudoc = GRU(self.units,\n",
        "                      return_sequences=False,\n",
        "                      return_state=False,\n",
        "                      recurrent_initializer='glorot_uniform')\n",
        "    self.dense = Dense(2, activation=tf.nn.relu)\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    # Should apply document embedding \n",
        "    x = self.embedding(x)\n",
        "    output = self.grudoc(x, initial_state = hidden)\n",
        "    output = self.dense(output)\n",
        "    return output\n",
        "  def initialize_hidden_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kCjuGZhz2IZ",
        "colab_type": "code",
        "outputId": "f818ce76-b1f3-4238-888e-9ffccd7dfa98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#testing the model\n",
        "gruDocModel = GruDoc(vocabularySize, embedding_size)\n",
        "gru_doc_h_0 = gruDocModel.initialize_hidden_state(Xtest.shape[0])\n",
        "print('Initial Gru Doc h_0', Xtest.shape)\n",
        "print('gru Sent last Hidden', )\n",
        "gru_doc_output = gruDocModel(h_L, gru_doc_h_0)\n",
        "print('output', gru_doc_output.shape)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial Gru Doc h_0 (36, 200)\n",
            "gru Sent last Hidden\n",
            "output (36, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWq7rHkUT7Gh",
        "colab_type": "code",
        "outputId": "fa618188-37ef-4800-ad4d-ff025be6b814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def transformToVector(ClusterSentences):\n",
        "  xs_cluster=[]\n",
        "  for sentences in ClusterSentences:\n",
        "    xs = []\n",
        "    for sentence in sentences:\n",
        "      words = [x.lower() for x in sentence.split()]\n",
        "      wids = [word_index[word] for word in words]\n",
        "      xs.append(wids)\n",
        "    xs_pad = tf.keras.preprocessing.sequence.pad_sequences(xs, maxlen=sentenceLength, padding='post')  \n",
        "    xs_cluster.append(xs_pad)\n",
        "  return xs_cluster\n",
        "XS_train = transformToVector(XClusterTrain)\n",
        "YS_train = []\n",
        "for clusterY in YClusterTrain:\n",
        "  ys = []\n",
        "  for sentenceY in clusterY:\n",
        "    ys.append(sentenceY)\n",
        "  YS_train.append(ys)\n",
        "print(len(XS_train))\n",
        "print(len(YS_train))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEhxe1WkWvwr",
        "colab_type": "code",
        "outputId": "9c33e656-e411-43d2-92b7-17e5b58f95a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "XS_test = transformToVector(XClusterTest)\n",
        "YS_test = []\n",
        "for clusterY in YClusterTest:\n",
        "  ys = []\n",
        "  for sentenceY in clusterY:\n",
        "    ys.append(sentenceY)\n",
        "  YS_test.append(ys)\n",
        "print(len(XS_test))\n",
        "print(len(YS_test))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJeludkmRV8z",
        "colab_type": "code",
        "outputId": "1d1dce0f-2c54-4d8e-a586-7b18033794c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "XS_train_sentences = []\n",
        "for xs_cluster in XS_train:\n",
        "  for xs_sentence in xs_cluster:\n",
        "    XS_train_sentences.append(xs_sentence)\n",
        "\n",
        "XS_test_sentences = []\n",
        "for xs_cluster in XS_test:\n",
        "  for xs_sentence in xs_cluster:\n",
        "    XS_test_sentences.append(xs_sentence)\n",
        "\n",
        "YS_train_labels = []\n",
        "for ys_cluster in YS_train:\n",
        "  for ys_label in ys_cluster:\n",
        "    YS_train_labels.append(ys_label)\n",
        "\n",
        "YS_test_labels = []\n",
        "for ys_cluster in YS_test:\n",
        "  for ys_label in ys_cluster:\n",
        "    YS_test_labels.append(ys_label) \n",
        "\n",
        "print('XS_train_sentences', len(XS_train_sentences))\n",
        "print('XS_test_sentences', len(XS_test_sentences))\n",
        "print('YS_train_labels', len(YS_train_labels))\n",
        "print('YS_test_labels', len(YS_test_labels)) \n",
        "YS_train_labels = np.array(YS_train_labels)\n",
        "YS_train_labels = convert_to_one_hot(YS_train_labels, C = 2)\n",
        "XS_train_sentences = tf.convert_to_tensor(XS_train_sentences)\n",
        "XS_test_sentences = tf.convert_to_tensor(XS_test_sentences)\n",
        "YS_test_labels = np.array(YS_test_labels)\n",
        "YS_test_labels = convert_to_one_hot(YS_test_labels, C = 2)\n",
        "\n",
        "\n",
        "print('XS train sentences shape', XS_train_sentences.shape)\n",
        "print('YS train labels shape', YS_train_labels.shape)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XS_train_sentences 1106\n",
            "XS_test_sentences 684\n",
            "YS_train_labels 1106\n",
            "YS_test_labels 684\n",
            "XS train sentences shape (1106, 200)\n",
            "YS train labels shape (1106, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7H78DIexLOh",
        "colab_type": "text"
      },
      "source": [
        "**Define The Optimazer and The Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSHru5dHQvnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.CategoricalCrossentropy(name='train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.CategoricalCrossentropy(name='test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndau1XpAqhaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(predicted_y, target_y):\n",
        "  \n",
        "  return tf.reduce_mean(tf.square(predicted_y - target_y))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_7XjHfGnLur",
        "colab_type": "text"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPdC_EFZREdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(sentences, labels, initial_hidden):\n",
        "  with tf.GradientTape() as tape:\n",
        "    output, h_L = gruSentModel(sentences, initial_hidden)\n",
        "    predictions = gruDocModel(h_L, initial_hidden)\n",
        "    loss = loss_object(labels, predictions)\n",
        "    \n",
        "  gradients = tape.gradient(loss, gruDocModel.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, gruDocModel.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03gUKFu9RMt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def test_step(sentences, labels, initial_hidden):\n",
        "  output, h_L  = gruSentModel(sentences, initial_hidden)\n",
        "  predictions = gruDocModel(h_L, initial_hidden)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "  \n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmhLrR_A5Ngv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((XS_train_sentences, YS_train_labels)).batch(10)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((XS_test_sentences, YS_test_labels)).batch(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STwhKYOFYRl_",
        "colab_type": "code",
        "outputId": "21b86fe4-8c3f-41a0-c239-3da4da11eaad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for sents, labels in train_ds:\n",
        "    #print('sents', sents.shape)\n",
        "    initial_hidden = gruSentModel.initialize_hidden_state(sents.shape[0])\n",
        "    train_step(sents, labels, initial_hidden)\n",
        "\n",
        "  for test_sentences, test_labels in test_ds:\n",
        "    initial_hidden = gruSentModel.initialize_hidden_state(test_sentences.shape[0])\n",
        "    test_step(test_sentences, test_labels, initial_hidden)\n",
        "  \n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "\n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 15.580809593200684, Accuracy: 1557.8873291015625, Test Loss: 15.300514221191406, Test Accuracy: 1529.3338623046875\n",
            "Epoch 2, Loss: 15.580809593200684, Accuracy: 1557.8873291015625, Test Loss: 15.300514221191406, Test Accuracy: 1529.3338623046875\n",
            "Epoch 3, Loss: 15.580809593200684, Accuracy: 1557.8873291015625, Test Loss: 15.300514221191406, Test Accuracy: 1529.3338623046875\n",
            "Epoch 4, Loss: 15.580809593200684, Accuracy: 1557.8873291015625, Test Loss: 15.300514221191406, Test Accuracy: 1529.3338623046875\n",
            "Epoch 5, Loss: 15.580809593200684, Accuracy: 1557.8873291015625, Test Loss: 15.300514221191406, Test Accuracy: 1529.3338623046875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvrlQJ-MIrh8",
        "colab_type": "text"
      },
      "source": [
        "**Summarizing**\n",
        "\n",
        "**Salience Scoring**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmj6uGZUIveL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clusterTransformToVector(ClusterSentences):\n",
        "  xs=[]\n",
        "  for sentences in ClusterSentences:\n",
        "    words = [x.lower() for x in sentences.split()]\n",
        "    wids = [word_index[word] for word in words]\n",
        "    xs.append(wids)\n",
        "  xs_pad = tf.keras.preprocessing.sequence.pad_sequences(xs, maxlen=sentenceLength, padding='post')\n",
        "  return xs_pad\n",
        "def salienceEstimation(clusterDocument):\n",
        "  cluster_doc_vector = clusterTransformToVector(clusterDocument)\n",
        "  print('cluster_doc_vector', cluster_doc_vector.shape)\n",
        "  initial_hidden = gruSentModel.initialize_hidden_state(cluster_doc_vector.shape[0])\n",
        "  print('initial_hidden', initial_hidden.shape)\n",
        "  output, h_L = gruSentModel(cluster_doc_vector, initial_hidden)\n",
        "  predictions = gruDocModel(h_L, initial_hidden)\n",
        "  sentence_list = []\n",
        "  salience_score_list = {}\n",
        "  print(len(cluster_doc_vector))\n",
        "  for i in range(len(cluster_doc_vector)):\n",
        "    prediction = predictions[i]\n",
        "    word_list = []\n",
        "    \n",
        "    for idx in cluster_doc_vector[i]:\n",
        "      if idx != 0 :\n",
        "        word_list.append(index_word[idx])\n",
        "    word_list = ' '.join(word_list)\n",
        "    salience_score_list[word_list] = prediction[1]\n",
        "    sentence_list.append(word_list)\n",
        "  return salience_score_list, sentence_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EDE6t2fyxo4",
        "colab_type": "code",
        "outputId": "b5c86922-0a40-473e-f084-dab22e39677c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "salience_scores, sentence_list = salienceEstimation(clusterSentences[0])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cluster_doc_vector (684, 200)\n",
            "initial_hidden (684, 300)\n",
            "684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu4xgxbRaenu",
        "colab_type": "text"
      },
      "source": [
        "**Sentences Selection And Summarization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsmkD36_Qzom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "500d76f4-f524-4fc2-8a62-49438c9e9227"
      },
      "source": [
        "ordering = sorted(salience_scores, key=lambda k: salience_scores[k],  reverse=True)\n",
        "summary_list = []\n",
        "sentence_length = 0\n",
        "for sent in ordering:\n",
        "  length = len(sent.split())\n",
        "  if (length > 5 and length < 55):\n",
        "    if sentence_length <= 88:\n",
        "      summary_list.append(sent)\n",
        "      sentence_length += length\n",
        "summary = ' '.join(summary_list)\n",
        "print('Predicted Summary:')\n",
        "print(summary)\n",
        "print()\n",
        "print('Length of summary',len(summary.split()))\n",
        "print()\n",
        "print('The summary sentences salience score:')\n",
        "\n",
        "for sent in summary_list:\n",
        "  print(salience_scores[sent])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Summary:\n",
            "but thomas who has risen in republican ranks as an advocate of bootstrap conservatism would present a striking change from marshall a civilrights pioneer and an anchor of the courts declining liberal faction thomas barely half the age of the man whose seat he was named to fill came of age in the world that marshall helped create now the question is to what extent thomas would add weight to the courts new majority that appears willing to dismantle important parts of the legacy that marshall helped to build\n",
            "\n",
            "Length of summary 89\n",
            "\n",
            "The summary sentences salience score:\n",
            "tf.Tensor(0.017739216, shape=(), dtype=float32)\n",
            "tf.Tensor(0.017739216, shape=(), dtype=float32)\n",
            "tf.Tensor(0.017739216, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEtjLCxFP9uD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "58b1c511-13df-4e57-8991-130e755151ba"
      },
      "source": [
        "print('Original summary')\n",
        "print(clusterSummaries[0])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original summary\n",
            "President Bushs nomination of black conservative Clarence Thomas to replace the Supreme Courts first black Justice liberal Thurgood Marshall split the Senate down the middle Thomass opposition to affirmative action alienated civil rights activists while his Catholic upbringing and interest in the priesthood raised alarm in abortionrights groups The Judiciary Committee deadlocked 77 and the nomination was referred to the Senate without recommendation after extended televised hearings on charges of sexual harassment against the nominee Thomas was confirmed by a close 5248 vote but he commented that nothing could give him back his good name \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyQhLx9lCQT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "52af1048-7384-45f9-e2f3-a3e609f476cf"
      },
      "source": [
        "!pip install rouge\n",
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(summary, clusterSummaries[0])\n",
        "print('Rouge Scores')\n",
        "print(scores)\n",
        "print('Average Rouge Score')\n",
        "average = (scores[0]['rouge-1']['f'] + scores[0]['rouge-2']['f'])/2 \n",
        "print(average)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.6/dist-packages (0.3.2)\n",
            "Rouge Scores\n",
            "[{'rouge-1': {'f': 0.15714285220816343, 'p': 0.1774193548387097, 'r': 0.14102564102564102}, 'rouge-2': {'f': 0.022727267743415352, 'p': 0.024096385542168676, 'r': 0.021505376344086023}, 'rouge-l': {'f': 0.0835596453818637, 'p': 0.0967741935483871, 'r': 0.07692307692307693}}]\n",
            "Average Rouge Score\n",
            "0.08993505997578939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jfZi36iCvMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}