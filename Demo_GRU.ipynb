{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo-GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vErJHiISK_i4",
        "colab_type": "text"
      },
      "source": [
        "**Synchronize to Google Drive**\n",
        "\n",
        "This feature makes the google colab to connect to the google drive with mounting GDrive and defining the path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQhvn8KgsHSV",
        "colab_type": "code",
        "outputId": "3e9eee3f-a3f4-409e-d368-00b6594e670b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# Synchronize to google drive, define the root path\n",
        "import os\n",
        "\n",
        "google_colab  = True\n",
        "if google_colab == True:\n",
        "  #This statement used to pointing the google drive storage \n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  root_path = 'gdrive/My Drive/Colab Notebooks/'\n",
        "\n",
        "  #This statement is purposed to import library from the Drive\n",
        "  os.chdir('gdrive/My Drive/Colab Notebooks/')\n",
        "  "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xggShlKKmbXq",
        "colab_type": "code",
        "outputId": "db3c5bfd-1ab1-45d2-e8ad-734a146f26fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, GRU, Activation, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "#from keras.models import Model\n",
        "#from keras.layers import Dense, Input, Dropout, LSTM, Activation, Flatten, GRU, Concatenate\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "np.random.seed(1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prFCcPnEqbyc",
        "colab_type": "text"
      },
      "source": [
        "**Hyper Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXl1uHcXqf6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gcn_hidden_layer = 3\n",
        "hidden_size = 300\n",
        "gru_units = 300\n",
        "learning_rate = 0.001\n",
        "batch_size = 1\n",
        "clipnorm = 1.0\n",
        "stop_early = True\n",
        "embedding_size=300\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jldpyssVMSLB",
        "colab_type": "text"
      },
      "source": [
        "**Loading the dataset**\n",
        "\n",
        "Currently, The experiment is using the dataset from DUC2001 and DUC2002. It designed to support Extractive Summarization method in which extract all of documents through pre-processing and store all of sentences to dataset-duc2001 pkl and dataset-duc2002.pkl and each of sentence has labeled based on whether its sentence is similar or exist on the summaries (labeled 1) or no (labeled 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b4b707de-b279-4c36-a1c8-c915203f334b",
        "id": "mwkgFuLwH3lw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "import collections\n",
        "import pickle #data preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "nltk.download('punkt')\n",
        "NUM_WORDS=25000\n",
        "counter = collections.Counter()\n",
        "print('Dataset loading ...')\n",
        "#import postprocessing as pr #helper\n",
        "#Step 1 - Load data\n",
        "#Step 1 - Load data\n",
        "with open('datasets/dataset-duc2001.pkl', 'rb') as fp:\n",
        "    data_2001 = pickle.load(fp)\n",
        "\n",
        "with open('datasets/dataset-duc2002.pkl', 'rb') as fp:\n",
        "    data_2002 = pickle.load(fp)\n",
        "\n",
        "clusterSentences_2001, clusterDocuments_2001, clusterSummaries_2001, listY_2001 = data_2001\n",
        "clusterSentences_2002, clusterDocuments_2002, clusterSummaries_2002, listY_2002 = data_2002\n",
        "\n",
        "clusterSentences = clusterSentences_2001 + clusterSentences_2002\n",
        "clusterDocuments = clusterDocuments_2001 + clusterDocuments_2002\n",
        "listY = listY_2001 + listY_2002\n",
        "\n",
        "# Extract to become one bucket of sentences\n",
        "sentences = []\n",
        "for clusterSentence in clusterSentences[:2]:\n",
        "  for sentence in clusterSentence:\n",
        "    sentences.append(sentence)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',lower=True)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "index_word = tokenizer.index_word\n",
        "index_docs = tokenizer.index_docs\n",
        "vocabularySize = len(word_index) + 1\n",
        "\n",
        "#nWord = [len(sentence.split()) for sentence in sentences]\n",
        "#nWord = sorted(nWord,  reverse=True)\n",
        "#words = [sentence.split() for sentence in sentences]\n",
        "#nWord = [len(sentence.split()) for sentence in sentences]\n",
        "#maxlen = max(nWord)\n",
        "sentenceLength = 200"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Dataset loading ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3Yrk8yZAOJM",
        "colab_type": "code",
        "outputId": "caf55853-1e15-4687-d1da-e0722129c305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print('total of words :', len(word_index))\n",
        "print('total of sentences', len(sentences))\n",
        "total_documents = 0\n",
        "for docs in clusterDocuments[:2]:\n",
        "  total_documents += len(docs)\n",
        "print('total of documents', total_documents)\n",
        "print('total of clusters', len(clusterSentences[:2]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total of words : 5666\n",
            "total of sentences 1790\n",
            "total of documents 22\n",
            "total of clusters 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rCPNnRVSZIM",
        "colab_type": "text"
      },
      "source": [
        "**Load Pre-trained Word2Vec**\n",
        "\n",
        "We use gensim to load Word2Vec which is provided Google. It can be downloaded from https://code.google.com/archive/p/word2vec/\n",
        "\n",
        "Based on the reference paper, the model use 300-dimensional pre-trained word2vec embeddings (Mikolov et al., 2013) as input to GRU$^{sent}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW7ocOcLs1q6",
        "colab_type": "code",
        "outputId": "a2f41688-c086-491f-faf4-959328daef41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "WORD2VEC_MODEL = \"word-embeddings/word2vec/GoogleNews-vectors-negative300.bin.gz\"\n",
        "word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_MODEL, binary=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3jANP3eDAeR",
        "colab_type": "text"
      },
      "source": [
        "**Define Y target**\n",
        "\n",
        "Y target defined as categorical data but ideally it should be binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00p3_zy00TNY",
        "colab_type": "code",
        "outputId": "370f7505-df10-4130-f67e-78f77e7d12c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)]\n",
        "    return Y\n",
        "    \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "xs, ys = [], []\n",
        "for sentence in sentences:\n",
        "  words = [x.lower() for x in sentence.split()]\n",
        "  wids = [word_index[word] for word in words]\n",
        "  xs.append(wids)\n",
        "\n",
        "ys = []\n",
        "for clusterY in listY[:2]:\n",
        "  for sentenceY in clusterY:\n",
        "    ys.append(sentenceY)\n",
        "\n",
        "X = pad_sequences(xs, maxlen=sentenceLength)\n",
        "Y = np.array(ys)\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.02, random_state=1)\n",
        "\n",
        "#if we use categorical mode\n",
        "Y_train_oh = convert_to_one_hot(Ytrain, C = 2)\n",
        "Y_test_oh = convert_to_one_hot(Ytest, C = 2)\n",
        "\n",
        "\n",
        "#create training and test data based on the clustering document\n",
        "XClusterTrain, XClusterTest, YClusterTrain, YClusterTest = train_test_split(clusterSentences[:2], listY[:2], test_size=0.5, random_state=1)\n",
        "print('XClusterTrain', len(XClusterTrain))\n",
        "print('YClusterTrain', len(YClusterTrain))\n",
        "print('XClusterTest', len(XClusterTest))\n",
        "print('YClusterTest',len(XClusterTest))\n",
        "### if want to reshape the target #############################################\n",
        "#Y_train_oh = Y_train_oh.reshape(Y_train_oh.shape[0], Y_train_oh.shape[1], 1)\n",
        "#Y_test_oh = Y_test_oh.reshape(Y_test_oh.shape[0], Y_test_oh.shape[1], 1)\n",
        "#print(Y_train_oh[0])\n",
        "#print(Y_test_oh[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XClusterTrain 1\n",
            "YClusterTrain 1\n",
            "XClusterTest 1\n",
            "YClusterTest 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmG7pJGJUsOP",
        "colab_type": "text"
      },
      "source": [
        "**Embedding Layer**\n",
        "\n",
        "This function is used to embed pre-trained word2vec as defined before, the function is to produce the matrix from all of the sentences which were mapped to word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7VbvoD5uhoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: pretrained_embedding_layer\n",
        "def pretrained_embedding_matrix(word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
        "    \n",
        "    Arguments:\n",
        "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    embedding_layer -- pretrained layer Keras instance\n",
        "    \"\"\"\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
        "    embedding_matrix = np.zeros((vocab_len, emb_dim))\n",
        "    \n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        try:\n",
        "          embedding_matrix[index, :] = word_to_vec_map[word]\n",
        "        except:\n",
        "          embedding_matrix[index, :] = 0\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G72hOeIIybu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_layer = pretrained_embedding_matrix(word2vec, word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sytdg3VVGUK",
        "colab_type": "text"
      },
      "source": [
        "**GRU$^{sent}$**: Sentence Embedding\n",
        "\n",
        "This model used to produce sentence embedding from a cluster document with $ N $sentences (s$_1$, s$_2$, ... , s$_N$) in total. for each sentence s$_i$ of $ L $ words (w$_1$, w$_2$, ..., w$_L$), GRU$^sent$ recurrently updates hidden states at each time step t:\n",
        "\n",
        "h$_t ^{sent} = GRU^{sent} h_{t-1} ^{sent} . w_t $ ......... (4)\n",
        "\n",
        "where w$_t$ is the word embedding for w$_t$, h$_t ^{sent}$ is the hidden state of GRU$^{sent}$. h$_0$ is initialized as a zero vector, and the input sentence embedding x$_i$ is the last hidden state :\n",
        "\n",
        "x$_i = h_L ^{sent}$ ....................................... (5)\n",
        "\n",
        "All sentence embeddings from the given document cluster are grouped as the node feature matrix X :\n",
        "\n",
        "$X = \n",
        "\\quad\n",
        "\\begin{bmatrix}\n",
        "- X_1 - \\\\\n",
        "- X_2 - \\\\\n",
        ". \\\\\n",
        ". \\\\\n",
        ". \\\\\n",
        "- X_N - \n",
        "\\end{bmatrix}$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnKjpDBcBJ8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "class GruSent(tf.keras.Model):\n",
        "  def __init__(self, vocabulary_size, embedding_size, pretrained_embedding):\n",
        "    super(GruSent, self).__init__()\n",
        "    self.vocabulary_size = vocabulary_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.pretrained_embedding = pretrained_embedding\n",
        "    self.batch_size = batch_size\n",
        "    self.units = gru_units\n",
        "    self.embedding = Embedding(self.vocabulary_size, self.embedding_size, weights=[self.pretrained_embedding], trainable=False)\n",
        "    self.grusent = GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.dense = Dense(1, activation=tf.nn.relu)\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state_h = self.grusent(x, initial_state = hidden)\n",
        "    output = self.dense(output)\n",
        "    return output, state_h\n",
        "  def initialize_hidden_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmQntutEgy9U",
        "colab_type": "code",
        "outputId": "9baaa68f-666f-4318-c5fa-dd2db50cae18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "gruSentModel = GruSent(vocabularySize, embedding_size, embedding_layer)\n",
        "gru_sent_h_0 = gruSentModel.initialize_hidden_state(Xtest.shape[0])\n",
        "print('Xtest', Xtest.shape)\n",
        "print('Initial Gru Sent Hidden', gru_sent_h_0.shape)\n",
        "output, h_L = gruSentModel(Xtest, gru_sent_h_0)\n",
        "print('output', output.shape)\n",
        "print('h_L', h_L.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xtest (36, 200)\n",
            "Initial Gru Sent Hidden (36, 300)\n",
            "output (36, 200, 1)\n",
            "h_L (36, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-j905HopxKv",
        "colab_type": "text"
      },
      "source": [
        "**Cluster Embedding**\n",
        "\n",
        "The model apply a second-level RNN $GRU^{doc}$ to encode the entire document cluster. Given a document cluster $C$ with $M$ documents $(d_1, d_2, ..., d_M)$, for document $d_i$ with $|d_i|$ sentences, $GRU^{doc}$ first builds the document embedding $d_i$ on top of sentence embeddings:\n",
        "\n",
        "$h_t^{doc} = GRU^{doc}(h_{t-1}^{doc}, s_t)$ ...... (8)\n",
        "\n",
        "$d_i = h_{d_i}^{doc} $ ..................................... (9)\n",
        "\n",
        "where $s_t$ is the sentence embedding in the document d_i. In Eq. 9, we extract the last hidden state as the document embedding for $d_i$. \n",
        "\n",
        "In Eq. 10, we average over document embeddings to produce the cluster embedding $C$\n",
        "\n",
        "$C = \\frac1M $$\\sum_{i=1}^M d_i$ ......................... (10)\n",
        "\n",
        "All the GRU used are forward.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_o3-s-7punY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "class GruDoc(tf.keras.Model):\n",
        "  def __init__(self, vocabularySize, embedding_size):\n",
        "    super(GruDoc, self).__init__()\n",
        "    self.vocabulary_size = vocabularySize\n",
        "    self.embedding_size = embedding_size\n",
        "    self.units = gru_units\n",
        "    self.embedding = Embedding(self.vocabulary_size, self.embedding_size, trainable=False)\n",
        "    self.grudoc = GRU(self.units,\n",
        "                      return_sequences=False,\n",
        "                      return_state=False,\n",
        "                      recurrent_initializer='glorot_uniform')\n",
        "    self.dense = Dense(1, activation=tf.nn.relu)\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    # Should apply document embedding \n",
        "    x = self.embedding(x)\n",
        "    output = self.grudoc(x, initial_state = hidden)\n",
        "    output = self.dense(output)\n",
        "    return output\n",
        "  def initialize_hidden_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kCjuGZhz2IZ",
        "colab_type": "code",
        "outputId": "79aca64e-a869-4c0e-f709-8fdd3d40dce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#testing the model\n",
        "gruDocModel = GruDoc(vocabularySize, embedding_size)\n",
        "gru_doc_h_0 = gruDocModel.initialize_hidden_state(Xtest.shape[0])\n",
        "print('Initial Gru Doc h_0', Xtest.shape)\n",
        "print('gru Sent last Hidden', )\n",
        "gru_doc_output = gruDocModel(h_L, gru_doc_h_0)\n",
        "print('output', gru_doc_output.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial Gru Doc h_0 (36, 200)\n",
            "gru Sent last Hidden\n",
            "output (36, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWq7rHkUT7Gh",
        "colab_type": "code",
        "outputId": "dc12baa7-cf22-4adb-c6e2-0dbb66137bde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def transformToVector(ClusterSentences):\n",
        "  xs_cluster=[]\n",
        "  for sentences in ClusterSentences:\n",
        "    xs = []\n",
        "    for sentence in sentences:\n",
        "      words = [x.lower() for x in sentence.split()]\n",
        "      wids = [word_index[word] for word in words]\n",
        "      xs.append(wids)\n",
        "    xs_pad = tf.keras.preprocessing.sequence.pad_sequences(xs, maxlen=sentenceLength, padding='post')  \n",
        "    xs_cluster.append(xs_pad)\n",
        "  return xs_cluster\n",
        "XS_train = transformToVector(XClusterTrain)\n",
        "YS_train = []\n",
        "for clusterY in YClusterTrain:\n",
        "  ys = []\n",
        "  for sentenceY in clusterY:\n",
        "    ys.append(sentenceY)\n",
        "  YS_train.append(ys)\n",
        "print(len(XS_train))\n",
        "print(len(YS_train))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEhxe1WkWvwr",
        "colab_type": "code",
        "outputId": "e84929f3-8be8-4713-806e-4f0a27d75690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "XS_test = transformToVector(XClusterTest)\n",
        "YS_test = []\n",
        "for clusterY in YClusterTest:\n",
        "  ys = []\n",
        "  for sentenceY in clusterY:\n",
        "    ys.append(sentenceY)\n",
        "  YS_test.append(ys)\n",
        "print(len(XS_test))\n",
        "print(len(YS_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJeludkmRV8z",
        "colab_type": "code",
        "outputId": "49c12ea1-fc1c-4134-e462-82422d2ca145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "XS_train_sentences = []\n",
        "for xs_cluster in XS_train:\n",
        "  for xs_sentence in xs_cluster:\n",
        "    XS_train_sentences.append(xs_sentence)\n",
        "\n",
        "XS_test_sentences = []\n",
        "for xs_cluster in XS_test:\n",
        "  for xs_sentence in xs_cluster:\n",
        "    XS_test_sentences.append(xs_sentence)\n",
        "\n",
        "YS_train_labels = []\n",
        "for ys_cluster in YS_train:\n",
        "  for ys_label in ys_cluster:\n",
        "    YS_train_labels.append(ys_label)\n",
        "\n",
        "YS_test_labels = []\n",
        "for ys_cluster in YS_test:\n",
        "  for ys_label in ys_cluster:\n",
        "    YS_test_labels.append(ys_label) \n",
        "\n",
        "print('XS_train_sentences', len(XS_train_sentences))\n",
        "print('XS_test_sentences', len(XS_test_sentences))\n",
        "print('YS_train_labels', len(YS_train_labels))\n",
        "print('YS_test_labels', len(YS_test_labels)) \n",
        "YS_train_labels = tf.convert_to_tensor(YS_train_labels)\n",
        "XS_train_sentences = tf.convert_to_tensor(XS_train_sentences)\n",
        "XS_test_sentences = tf.convert_to_tensor(XS_test_sentences)\n",
        "YS_test_labels = tf.convert_to_tensor(YS_test_labels)\n",
        "print('XS train sentences shape', XS_train_sentences.shape)\n",
        "print('YS train labels shape', YS_train_labels.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XS_train_sentences 1106\n",
            "XS_test_sentences 684\n",
            "YS_train_labels 1106\n",
            "YS_test_labels 684\n",
            "XS train sentences shape (1106, 200)\n",
            "YS train labels shape (1106,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7H78DIexLOh",
        "colab_type": "text"
      },
      "source": [
        "**Define The Optimazer and The Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSHru5dHQvnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_7XjHfGnLur",
        "colab_type": "text"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPdC_EFZREdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(sentences, labels, initial_hidden):\n",
        "  with tf.GradientTape() as tape:\n",
        "    output, h_L = gruSentModel(sentences, initial_hidden)\n",
        "    predictions = gruDocModel(h_L, initial_hidden)\n",
        "    loss = loss_object(labels, predictions)\n",
        "    \n",
        "  gradients = tape.gradient(loss, gruDocModel.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, gruDocModel.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03gUKFu9RMt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def test_step(sentences, labels, initial_hidden):\n",
        "  output, h_L  = gruSentModel(sentences, initial_hidden)\n",
        "  predictions = gruDocModel(h_L, initial_hidden)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "  \n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmhLrR_A5Ngv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((XS_train_sentences, YS_train_labels)).batch(10)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((XS_test_sentences, YS_test_labels)).batch(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STwhKYOFYRl_",
        "colab_type": "code",
        "outputId": "6d818e71-e26f-457d-c45d-8eaf708c74cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for sents, labels in train_ds:\n",
        "    #print('sents', sents.shape)\n",
        "    initial_hidden = gruSentModel.initialize_hidden_state(sents.shape[0])\n",
        "    train_step(sents, labels, initial_hidden)\n",
        "\n",
        "  for test_sentences, test_labels in test_ds:\n",
        "    initial_hidden = gruSentModel.initialize_hidden_state(test_sentences.shape[0])\n",
        "    test_step(test_sentences, test_labels, initial_hidden)\n",
        "  \n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "\n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1, Loss: 0.5141651630401611, Accuracy: 96.66667938232422, Test Loss: 0.7824252247810364, Test Accuracy: 94.92755889892578\n",
            "Epoch 2, Loss: 0.5141651630401611, Accuracy: 96.66667938232422, Test Loss: 0.7824252247810364, Test Accuracy: 94.92755889892578\n",
            "Epoch 3, Loss: 0.5141651630401611, Accuracy: 96.66667938232422, Test Loss: 0.7824252247810364, Test Accuracy: 94.92755889892578\n",
            "Epoch 4, Loss: 0.5141651630401611, Accuracy: 96.66667938232422, Test Loss: 0.7824252247810364, Test Accuracy: 94.92755889892578\n",
            "Epoch 5, Loss: 0.5141651630401611, Accuracy: 96.66667938232422, Test Loss: 0.7824252247810364, Test Accuracy: 94.92755889892578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvrlQJ-MIrh8",
        "colab_type": "text"
      },
      "source": [
        "**Summarizing**\n",
        "\n",
        "**Salience Scoring**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmj6uGZUIveL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clusterTransformToVector(ClusterSentences):\n",
        "  xs=[]\n",
        "  for sentences in ClusterSentences:\n",
        "    words = [x.lower() for x in sentence.split()]\n",
        "    wids = [word_index[word] for word in words]\n",
        "    xs.append(wids)\n",
        "  xs_pad = tf.keras.preprocessing.sequence.pad_sequences(xs, maxlen=sentenceLength, padding='post')\n",
        "  return xs_pad\n",
        "def salienceEstimation(clusterDocument):\n",
        "  cluster_doc_vector = clusterTransformToVector(clusterDocument)\n",
        "  print('cluster_doc_vector', cluster_doc_vector.shape)\n",
        "  initial_hidden = gruSentModel.initialize_hidden_state(cluster_doc_vector.shape[0])\n",
        "  print('initial_hidden', initial_hidden.shape)\n",
        "  output, h_L = gruSentModel(cluster_doc_vector, initial_hidden)\n",
        "  predictions = gruDocModel(h_L, initial_hidden)\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EDE6t2fyxo4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "25c28bcd-8d26-41e4-c2b5-aad047dc3338"
      },
      "source": [
        "salience_scores = salienceEstimation(clusterSentences[2])\n",
        "print(salience_scores.shape)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cluster_doc_vector (393, 200)\n",
            "initial_hidden (393, 300)\n",
            "(393, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu4xgxbRaenu",
        "colab_type": "text"
      },
      "source": [
        "**Sentences Selection And Summarization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsmkD36_Qzom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}