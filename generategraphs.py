# -*- coding: utf-8 -*-
"""GenerateGraphs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w8eeHUn7YOfZ2wwYcCvk4cje-Z5wPfIO
"""

# Synchronize to google drive, define the root path
import os

google_colab  = True
if google_colab == True:
  #This statement used to pointing the google drive storage 
  from google.colab import drive
  drive.mount('/content/gdrive')
  root_path = 'gdrive/My Drive/Colab Notebooks/'

  #This statement is purposed to import library from the Drive
  os.chdir('gdrive/My Drive/Colab Notebooks/')

import tensorflow as tf
tf.enable_eager_execution()
import numpy as np
np.random.seed(0)
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, GRU, Activation, Flatten
from tensorflow.keras.models import Model
#from keras.models import Model
#from keras.layers import Dense, Input, Dropout, LSTM, Activation, Flatten, GRU, Concatenate
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.callbacks import EarlyStopping
np.random.seed(1)

import nltk
import collections
import pickle #data preprocessing
from tensorflow.keras.preprocessing.text import Tokenizer
nltk.download('punkt')
NUM_WORDS=25000
counter = collections.Counter()
print('Dataset loading ...')
#import postprocessing as pr #helper
#Step 1 - Load data
#Step 1 - Load data
with open('datasets/dataset-duc2001.pkl', 'rb') as fp:
    data_2001 = pickle.load(fp)

with open('datasets/dataset-duc2002.pkl', 'rb') as fp:
    data_2002 = pickle.load(fp)

clusterSentences_2001, clusterDocuments_2001, clusterSummaries_2001, listY_2001 = data_2001
clusterSentences_2002, clusterDocuments_2002, clusterSummaries_2002, listY_2002 = data_2002

clusterSentences = clusterSentences_2001 + clusterSentences_2002
clusterDocuments = clusterDocuments_2001 + clusterDocuments_2002
listY = listY_2001 + listY_2002

# Extract to become one bucket of sentences
sentences = []
for clusterSentence in clusterSentences:
  for sentence in clusterSentence:
    sentences.append(sentence)

tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n\'',lower=True)
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
index_word = tokenizer.index_word
index_docs = tokenizer.index_docs
vocabularySize = len(word_index) + 1

#nWord = [len(sentence.split()) for sentence in sentences]
#nWord = sorted(nWord,  reverse=True)
#words = [sentence.split() for sentence in sentences]
#nWord = [len(sentence.split()) for sentence in sentences]
#maxlen = max(nWord)
sentenceLength = 200

from lib.lexrank import lexrank
#from lib.simpleGCN import gcn
cluster_graph = []
lexrank_results = []
for clsSentences in clusterSentences:
  print(len(clsSentences))
  lx_sentences, lx_num, lxrank, graph_train = lexrank(clsSentences, word_split=True)
  cluster_graph.append(graph_train)
  lexrank_results.append(lxrank)

import pickle #data preprocessing
# Store multiple data on file .pkl
with open('datasets/graph-cosine_similarity.pkl', 'wb') as file_pi:
        pickle.dump((cluster_graph, lexrank_results), file_pi, -1)